{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [str(i).zfill(5) for i in range(2105)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realdata = []\n",
    "for i in range(2105):\n",
    "    with open (\"data/sparserep/part-\" + nums[i], \"r\") as myfile:\n",
    "        data=myfile.readlines()\n",
    "    realdata.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "realdata[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(realdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realdata[57080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(realdata[50].replace(\"List\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(realdata)):\n",
    "    try:\n",
    "        #data.append(eval(realdata[i][1:][:len(realdata[i])-3].replace(\"CompactBuffer\",\"\").replace(\",\\\"\",\",\\\"\").replace(\"), \",\"\\\"), \").replace(\"))\", \"\\\"))\").replace(\"\\\"\\\"\", \"\\\"\").replace(\",\\\")\",\",\\\"\\\")\").replace(\"((\",\"[(\").replace(\"))\",\")]\")))\n",
    "        data.append(eval(realdata[i].replace(\"List\",\"\")))\n",
    "    except SyntaxError:\n",
    "        print(\"error\" + str(i))\n",
    "#45307"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"readmission_labels.csv\")\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(data)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(labels, how = \"left\", left_on = 0, right_on = \"HADM_ID\")\n",
    "del df[\"HADM_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = 0\n",
    "testvec = ((365,62.0),(465,30))\n",
    "writestring = \"\"\n",
    "try:\n",
    "    sortedtestvec = tuple(sorted(testvec, key=lambda item: item[0]))\n",
    "\n",
    "\n",
    "    for i in sortedtestvec:\n",
    "        if i[0]>max:\n",
    "            max = i[0]\n",
    "        writestring = writestring + \" \" + str(i[0])+ \":\" + str(i[1])\n",
    "except TypeError:\n",
    "    if testvec[0]>max:\n",
    "            max = testvec[0]\n",
    "    writestring = writestring + \" \" + str(testvec[0])+ \":\" + str(testvec[1])\n",
    "\n",
    "writestring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_SORTED_TUPLE = tuple(sorted(testvec, key=lambda item: item[0]))\n",
    "MY_SORTED_TUPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svmlight(testvec, label):\n",
    "    writestring = \"0\" if label == 0 else \"1\"\n",
    "    try:\n",
    "        sortedtestvec = tuple(sorted(testvec, key=lambda item: item[0]))\n",
    "        for i in sortedtestvec:\n",
    "            if i[0] not in {34031, 37927, 23502, 23523, 1461, 1459}:\n",
    "                writestring = writestring + \" \" + str(i[0])+ \":\" + str(i[1])\n",
    "    except TypeError:\n",
    "        writestring = writestring + \" \" + str(testvec[0])+ \":\" + str(testvec[1])\n",
    "    return writestring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"svmlight\"] = np.vectorize(svmlight)(df[1],df[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"svmlight\"]].to_csv(\"data.svmlight\", index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = pd.read_csv(\"data.svmlight\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl[1] = sl[0].apply(lambda x: x[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl[2] = sl[1].apply(lambda x: [i.split(\":\") for i in x.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl[3]= sl[2].apply(lambda x: [i[0] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl[4] = sl[3].apply(lambda x: np.unique(x).size == len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl[sl[4] == False][3].apply(lambda x: [item for item, count in collections.Counter(x).items() if count > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "data scaled\n",
      "defined network\n",
      "starting training\n",
      "[1,   100] loss: 0.571\n",
      "[1,   200] loss: 0.430\n",
      "[1,   300] loss: 0.343\n",
      "[1,   400] loss: 0.287\n",
      "[1,   500] loss: 0.266\n",
      "[1,   600] loss: 0.225\n",
      "[1,   700] loss: 0.225\n",
      "[1,   800] loss: 0.213\n",
      "[1,   900] loss: 0.235\n",
      "[1,  1000] loss: 0.227\n",
      "[1,  1100] loss: 0.242\n",
      "[1,  1200] loss: 0.205\n",
      "[1,  1300] loss: 0.217\n",
      "[1,  1400] loss: 0.203\n",
      "[1,  1500] loss: 0.207\n",
      "[1,  1600] loss: 0.215\n",
      "[1,  1700] loss: 0.208\n",
      "[2,   100] loss: 0.213\n",
      "[2,   200] loss: 0.234\n",
      "[2,   300] loss: 0.230\n",
      "[2,   400] loss: 0.223\n",
      "[2,   500] loss: 0.236\n",
      "[2,   600] loss: 0.201\n",
      "[2,   700] loss: 0.213\n",
      "[2,   800] loss: 0.205\n",
      "[2,   900] loss: 0.231\n",
      "[2,  1000] loss: 0.223\n",
      "[2,  1100] loss: 0.238\n",
      "[2,  1200] loss: 0.201\n",
      "[2,  1300] loss: 0.213\n",
      "[2,  1400] loss: 0.199\n",
      "[2,  1500] loss: 0.205\n",
      "[2,  1600] loss: 0.213\n",
      "[2,  1700] loss: 0.204\n",
      "[3,   100] loss: 0.210\n",
      "[3,   200] loss: 0.230\n",
      "[3,   300] loss: 0.226\n",
      "[3,   400] loss: 0.221\n",
      "[3,   500] loss: 0.234\n",
      "[3,   600] loss: 0.198\n",
      "[3,   700] loss: 0.210\n",
      "[3,   800] loss: 0.202\n",
      "[3,   900] loss: 0.228\n",
      "[3,  1000] loss: 0.219\n",
      "[3,  1100] loss: 0.234\n",
      "[3,  1200] loss: 0.199\n",
      "[3,  1300] loss: 0.210\n",
      "[3,  1400] loss: 0.195\n",
      "[3,  1500] loss: 0.203\n",
      "[3,  1600] loss: 0.211\n",
      "[3,  1700] loss: 0.201\n",
      "[4,   100] loss: 0.207\n",
      "[4,   200] loss: 0.227\n",
      "[4,   300] loss: 0.222\n",
      "[4,   400] loss: 0.218\n",
      "[4,   500] loss: 0.233\n",
      "[4,   600] loss: 0.195\n",
      "[4,   700] loss: 0.208\n",
      "[4,   800] loss: 0.200\n",
      "[4,   900] loss: 0.226\n",
      "[4,  1000] loss: 0.217\n",
      "[4,  1100] loss: 0.231\n",
      "[4,  1200] loss: 0.197\n",
      "[4,  1300] loss: 0.208\n",
      "[4,  1400] loss: 0.193\n",
      "[4,  1500] loss: 0.202\n",
      "[4,  1600] loss: 0.210\n",
      "[4,  1700] loss: 0.199\n",
      "[5,   100] loss: 0.205\n",
      "[5,   200] loss: 0.225\n",
      "[5,   300] loss: 0.219\n",
      "[5,   400] loss: 0.217\n",
      "[5,   500] loss: 0.232\n",
      "[5,   600] loss: 0.193\n",
      "[5,   700] loss: 0.206\n",
      "[5,   800] loss: 0.199\n",
      "[5,   900] loss: 0.224\n",
      "[5,  1000] loss: 0.215\n",
      "[5,  1100] loss: 0.229\n",
      "[5,  1200] loss: 0.195\n",
      "[5,  1300] loss: 0.206\n",
      "[5,  1400] loss: 0.191\n",
      "[5,  1500] loss: 0.201\n",
      "[5,  1600] loss: 0.209\n",
      "[5,  1700] loss: 0.198\n",
      "[6,   100] loss: 0.203\n",
      "[6,   200] loss: 0.223\n",
      "[6,   300] loss: 0.217\n",
      "[6,   400] loss: 0.216\n",
      "[6,   500] loss: 0.231\n",
      "[6,   600] loss: 0.191\n",
      "[6,   700] loss: 0.204\n",
      "[6,   800] loss: 0.198\n",
      "[6,   900] loss: 0.223\n",
      "[6,  1000] loss: 0.213\n",
      "[6,  1100] loss: 0.227\n",
      "[6,  1200] loss: 0.194\n",
      "[6,  1300] loss: 0.204\n",
      "[6,  1400] loss: 0.189\n",
      "[6,  1500] loss: 0.201\n",
      "[6,  1600] loss: 0.208\n",
      "[6,  1700] loss: 0.196\n",
      "[7,   100] loss: 0.202\n",
      "[7,   200] loss: 0.221\n",
      "[7,   300] loss: 0.216\n",
      "[7,   400] loss: 0.214\n",
      "[7,   500] loss: 0.231\n",
      "[7,   600] loss: 0.190\n",
      "[7,   700] loss: 0.202\n",
      "[7,   800] loss: 0.197\n",
      "[7,   900] loss: 0.222\n",
      "[7,  1000] loss: 0.211\n",
      "[7,  1100] loss: 0.226\n",
      "[7,  1200] loss: 0.193\n",
      "[7,  1300] loss: 0.203\n",
      "[7,  1400] loss: 0.187\n",
      "[7,  1500] loss: 0.200\n",
      "[7,  1600] loss: 0.207\n",
      "[7,  1700] loss: 0.195\n",
      "[8,   100] loss: 0.201\n",
      "[8,   200] loss: 0.220\n",
      "[8,   300] loss: 0.214\n",
      "[8,   400] loss: 0.213\n",
      "[8,   500] loss: 0.230\n",
      "[8,   600] loss: 0.189\n",
      "[8,   700] loss: 0.201\n",
      "[8,   800] loss: 0.196\n",
      "[8,   900] loss: 0.221\n",
      "[8,  1000] loss: 0.210\n",
      "[8,  1100] loss: 0.224\n",
      "[8,  1200] loss: 0.192\n",
      "[8,  1300] loss: 0.202\n",
      "[8,  1400] loss: 0.186\n",
      "[8,  1500] loss: 0.199\n",
      "[8,  1600] loss: 0.206\n",
      "[8,  1700] loss: 0.195\n",
      "[9,   100] loss: 0.200\n",
      "[9,   200] loss: 0.218\n",
      "[9,   300] loss: 0.213\n",
      "[9,   400] loss: 0.212\n",
      "[9,   500] loss: 0.229\n",
      "[9,   600] loss: 0.188\n",
      "[9,   700] loss: 0.200\n",
      "[9,   800] loss: 0.195\n",
      "[9,   900] loss: 0.220\n",
      "[9,  1000] loss: 0.209\n",
      "[9,  1100] loss: 0.223\n",
      "[9,  1200] loss: 0.191\n",
      "[9,  1300] loss: 0.201\n",
      "[9,  1400] loss: 0.185\n",
      "[9,  1500] loss: 0.199\n",
      "[9,  1600] loss: 0.205\n",
      "[9,  1700] loss: 0.194\n",
      "[10,   100] loss: 0.199\n",
      "[10,   200] loss: 0.217\n",
      "[10,   300] loss: 0.212\n",
      "[10,   400] loss: 0.211\n",
      "[10,   500] loss: 0.229\n",
      "[10,   600] loss: 0.187\n",
      "[10,   700] loss: 0.198\n",
      "[10,   800] loss: 0.194\n",
      "[10,   900] loss: 0.219\n",
      "[10,  1000] loss: 0.208\n",
      "[10,  1100] loss: 0.222\n",
      "[10,  1200] loss: 0.191\n",
      "[10,  1300] loss: 0.200\n",
      "[10,  1400] loss: 0.184\n",
      "[10,  1500] loss: 0.198\n",
      "[10,  1600] loss: 0.204\n",
      "[10,  1700] loss: 0.193\n",
      "Finished Training\n",
      "start testing\n",
      "finished testing\n",
      "AUC: 0.7016707166584594\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "X, y = load_svmlight_file(\"data.svmlight\")\n",
    "print(\"data loaded\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "scaler = MaxAbsScaler().fit(X)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "print(\"data scaled\")\n",
    "\n",
    "\n",
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(FeedForwardNet, self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_input, n_hidden)\n",
    "        self.hidden2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "net = FeedForwardNet(n_input=X.shape[1], n_hidden = 100, n_output=1)\n",
    "print(\"defined network\")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "print(\"starting training\")\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    i = 0\n",
    "    k = 0\n",
    "    batchsize = 25\n",
    "    while i<=X_train.shape[0]:\n",
    "        Xtrainsample = X_train_transformed.tocsr()[i:i+batchsize].todense()\n",
    "        ytrainsample = y_train[i:i+batchsize]\n",
    "\n",
    "        inputs = torch.from_numpy(Xtrainsample.astype('float32'))\n",
    "        labels = torch.from_numpy(ytrainsample.astype('float32')).view(-1,1)\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "\n",
    "        if k % 100 == 99:    # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, k + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        k = k+1\n",
    "        i = i+batchsize\n",
    "        \n",
    "print('Finished Training')\n",
    "print(\"start testing\")\n",
    "\n",
    "y_true = []\n",
    "y_scores = []\n",
    "\n",
    "i = 0\n",
    "while i<=X_test.shape[0]:\n",
    "    Xtestsample = X_test_transformed.tocsr()[i:i+batchsize].todense()\n",
    "    ytestsample = y_test[i:i+batchsize]\n",
    "\n",
    "\n",
    "    inputs = torch.from_numpy(Xtestsample.astype('float32'))\n",
    "    labels = torch.from_numpy(ytestsample.astype('float32')).view(-1,1)\n",
    "    \n",
    "    outputs = net(Variable(inputs))\n",
    "    outputs = F.sigmoid(outputs)\n",
    "    y_true.extend(labels.numpy().flatten().tolist())\n",
    "    y_scores.extend(outputs.data.numpy().flatten().tolist())\n",
    "    i = i + batchsize\n",
    "print(\"finished testing\")\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "auc_ffnet = auc(fpr, tpr)\n",
    "print(\"AUC: \" + str(auc_ffnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "X, y = load_svmlight_file(\"data.svmlight\")\n",
    "#X = X.toarray() # make it dense\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_test.copy()\n",
    "y_train = y_test.copy()\n",
    "X_test = X_test2.copy()\n",
    "y_test = y_test2.copy()\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "X = coo_matrix(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.tocsr()[58764:58769].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler().fit(X)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = coo_matrix(X_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(X_train_transformed.row), len(X_train_transformed.col)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# lets fix the random seeds for reproducibility.\n",
    "torch.manual_seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(0)\n",
    "\n",
    "X_train_transformed = coo_matrix(X_train_transformed)\n",
    "ixtr = torch.LongTensor([X_train_transformed.row.tolist(), X_train_transformed.col.tolist()])\n",
    "vxtr = torch.FloatTensor(X_train_transformed.data.tolist())\n",
    "\n",
    "trainset = TensorDataset(torch.sparse.FloatTensor(ixtr, vxtr,torch.Size(X_train_transformed.shape)).to_dense(), torch.from_numpy(y_train.astype('float32')).view(-1,1))\n",
    "#trainset = TensorDataset(torch.sparse.FloatTensor(ixtr, vxtr,torch.Size(X_train_transformed.shape)).to_dense(), torch.from_numpy(y_train.astype('float32')).view(-1,1))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "X_test_transformed = coo_matrix(X_test_transformed)\n",
    "ixte = torch.LongTensor([X_test_transformed.row.tolist(), X_test_transformed.col.tolist()])\n",
    "vxte = torch.FloatTensor(X_test_transformed.data.tolist())\n",
    "\n",
    "testset = TensorDataset(torch.sparse.FloatTensor(ixte, vxte,torch.Size(X_test_transformed.shape)).to_dense(), torch.from_numpy(y_test.astype('float32')).view(-1,1))\n",
    "#testset = TensorDataset(torch.sparse.FloatTensor(ixte, vxte,torch.Size(X_test_transformed.shape)), torch.from_numpy(y_test.astype('float32')).view(-1,1))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(FeedForwardNet, self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_input, n_hidden)\n",
    "        self.hidden2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "net = FeedForwardNet(n_input=38633, n_hidden = 100, n_output=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    i = 0\n",
    "    k = 0\n",
    "    batchsize = 4\n",
    "    while i<=X_train.shape[0]:\n",
    "        Xtrainsample = X_train_transformed.tocsr()[i:i+batchsize].todense()\n",
    "        ytrainsample = y_train[i:i+batchsize]\n",
    "\n",
    "\n",
    "        inputs = torch.from_numpy(Xtrainsample.astype('float32'))\n",
    "        labels = torch.from_numpy(ytrainsample.astype('float32')).view(-1,1)\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "\n",
    "        if k % 10 == 9:    # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, k + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        k = k+1\n",
    "        i = i+batchsize\n",
    "        \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_scores = []\n",
    "\n",
    "i = 0\n",
    "batchsize = 4\n",
    "while i<=X_test.shape[0]:\n",
    "    Xtestsample = X_test_transformed.tocsr()[i:i+batchsize].todense()\n",
    "    ytestsample = y_test[i:i+batchsize]\n",
    "\n",
    "\n",
    "    inputs = torch.from_numpy(Xtestsample.astype('float32'))\n",
    "    labels = torch.from_numpy(ytestsample.astype('float32')).view(-1,1)\n",
    "    \n",
    "    outputs = net(Variable(inputs))\n",
    "    outputs = F.sigmoid(outputs)\n",
    "    y_true.extend(labels.numpy().flatten().tolist())\n",
    "    y_scores.extend(outputs.data.numpy().flatten().tolist())\n",
    "    i = i + batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_scores = []\n",
    "\n",
    "for data in testloader:\n",
    "    inputs, labels = data\n",
    "    outputs = net(Variable(inputs))\n",
    "    outputs = F.sigmoid(outputs)\n",
    "    y_true.extend(labels.numpy().flatten().tolist())\n",
    "    y_scores.extend(outputs.data.numpy().flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "auc_ffnet = auc(fpr, tpr)\n",
    "auc_ffnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = df.iloc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"trial.csv\", index = False, quoting = 2)\n",
    "#go into data, replace quotes with blanks, why do they occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coo_matrix(([4,5,6],([0,0,0],[1,2,3])), shape = (1,5)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coo_matrix(([3],([0],[0])), shape = (1,1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dok_matrix([(365, 62.0), (1237, 3150.0)]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keys(x):\n",
    "    #print(list(x))\n",
    "    try:\n",
    "        return [i[0] for i in list(x)]\n",
    "    except TypeError:\n",
    "        return x[0]\n",
    "def vals(x):\n",
    "    #print(list(x))\n",
    "    try:\n",
    "        return [i[1] for i in list(x)]\n",
    "    except TypeError:\n",
    "        return x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[2] = df[1].apply(lambda x: keys(x))\n",
    "df[3] = df[1].apply(lambda x: vals(x))\n",
    "#np.vectorize(fun)(fd[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##everything below is unnecessary stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numitems = 226761\n",
    "addeditems = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"1\".isnumeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isfloat(value):\n",
    "  try:\n",
    "    float(value)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changetonums(x):\n",
    "    array = []\n",
    "    for j in x:\n",
    "        if isfloat(j[1]):\n",
    "            array.append((j[0],float(j[1])))\n",
    "        else:\n",
    "            array.append((j[0],j[1]))\n",
    "        #else:\n",
    "        #    if not j in addeditems.values():\n",
    "        #        addeditems[j] = numitems\n",
    "        #        numitems = numitems + 1\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd[3] = np.vectorize(changetonums)(fd[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addeditems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
